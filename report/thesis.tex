% do not change these two lines (this is a hard requirement
% there is one exception: you might replace oneside by twoside in case you deliver 
% the printed version in the accordant format
\documentclass[11pt,titlepage,oneside,openany]{book}
\usepackage{times}


\usepackage{graphicx}
\usepackage{latexsym}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{wasysym}

\usepackage[hyphens]{url}

\usepackage{ntheorem}

% \usepackage{paralist}
\usepackage{tabularx}


\usepackage[table,xcdraw]{xcolor}
\usepackage{pdflscape}
\usepackage{afterpage}
\usepackage{rotating}
\usepackage{makecell}
\usepackage{hyperref}

% this packaes are useful for nice algorithms
\usepackage{algorithm}
\usepackage{algorithmic}

% well, when your work is concerned with definitions, proposition and so on, we suggest this
% feel free to add Corrolary, Theorem or whatever you need
\newtheorem{definition}{Definition}
\newtheorem{proposition}{Proposition}

%table spacing
\newcolumntype{b}{X}
\newcolumntype{n}{>{\hsize=1.2\hsize}X}
\newcolumntype{s}{>{\hsize=.3\hsize}X}
\newcolumntype{m}{>{\hsize=.5\hsize}X}

% its always useful to have some shortcuts (some are specific for algorithms
% if you do not like your formating you can change it here (instead of scanning through the whole text)
\renewcommand{\algorithmiccomment}[1]{\ensuremath{\rhd} \textit{#1}}
\def\MYCALL#1#2{{\small\textsc{#1}}(\textup{#2})}
\def\MYSET#1{\scshape{#1}}
\def\MYAND{\textbf{ and }}
\def\MYOR{\textbf{ or }}
\def\MYNOT{\textbf{ not }}
\def\MYTHROW{\textbf{ throw }}
\def\MYBREAK{\textbf{break }}
\def\MYEXCEPT#1{\scshape{#1}}
\def\MYTO{\textbf{ to }}
\def\MYNIL{\textsc{Nil}}
\def\MYUNKNOWN{ unknown }
% simple stuff (not all of this is used in this examples thesis
\def\INT{{\mathcal I}} % interpretation
\def\ONT{{\mathcal O}} % ontology
\def\SEM{{\mathcal S}} % alignment semantic
\def\ALI{{\mathcal A}} % alignment
\def\USE{{\mathcal U}} % set of unsatisfiable entities
\def\CON{{\mathcal C}} % conflict set
\def\DIA{\Delta} % diagnosis
% mups and mips
\def\MUP{{\mathcal M}} % ontology
\def\MIP{{\mathcal M}} % ontology
% distributed and local entities
\newcommand{\cc}[2]{\mathit{#1}\hspace{-1pt} \# \hspace{-1pt} \mathit{#2}}
\newcommand{\cx}[1]{\mathit{#1}}
% complex stuff
\def\MER#1#2#3#4{#1 \cup_{#3}^{#2} #4} % merged ontology
\def\MUPALL#1#2#3#4#5{\textit{MUPS}_{#1}\left(#2, #3, #4, #5\right)} % the set of all mups for some concept
\def\MIPALL#1#2{\textit{MIPS}_{#1}\left(#2\right)} % the set of all mips





\begin{document}

\pagenumbering{roman}
% lets go for the title page, something like this should be okay
\begin{titlepage}
	\vspace*{2cm}
  \begin{center}
   {\Large The Extremes of Good and Evil\\}
   \vspace{2cm} 
   {Master Thesis\\}
   \vspace{2cm}
   {presented by\\
    Earl Hickey \\
    Matriculation Number 9083894\\
   }
   \vspace{1cm} 
   {submitted to the\\
    Data and Web Science Group\\
    Prof.\ Dr.\ Right Name Here\\
    University of Mannheim\\} \vspace{2cm}
   {August 2014}
  \end{center}
\end{titlepage} 

% no lets make some add some table of contents
\tableofcontents
\newpage

\listoffigures

\listoftables

% evntuelly you might add something like this
% \listtheorems{definition}
% \listtheorems{proposition}

\newpage


% okay, start new numbering ... here is where it really starts
\pagenumbering{arabic}

\chapter{Phase I - Data Translation}
\label{cha:data-translation}
 
\section{Use Case \& Data Profiling}
The goal of the project is to aggregate company information from several sources. To this end we used the suggestions from the project into slides as a foundation for our use case. We also included an additional source and amended the dbpedia query to extract further relevant information. Thus, the relevent entity will be a company.
We relied on 4 different datasources which are profiled in Table \ref{tab:dataset-overview}. In order to being able to process the kaggle dataset, we had to filter it down. To this end, we used the "size range" attribute and only kept the categories "10001+", "5001 - 10000", "1001 - 5000", "501 - 1000", "201 - 500", "51 - 200". This is a valid approach since the forbes dataset contains data about the 2000 largest companies in the world, and the dataworld (dw) dataset is also called "largest companies", containing only companies of a certain size.

\begin{table}[b]
	%TODO dbpedia query
	\label{tab:dataset-overview}
	\small
	\centering
	\begin{tabularx}{\textwidth}{mscccn}
		Dataset   & Source                                                       & Format                & \#E\textsuperscript{1}       & \#A \textsuperscript{2} & List Of Attributes                                                                                                                                                          \\\hline
		&&&&&\\
		kaggle    & \href{https://www.kaggle.com/kaleab1/companies}{link to dataset}                        & csv                  & \makecell[ct]{7.1M\\(491.830\textsuperscript{3})}                & 11               & ID, name, domain, year founded (MV), industry, size range, locality   (MV), country (MV), linkedin url, current employee estimate, total   employee estimate \\
		forbes    &\href{https://www.kaggle.com/ash316/forbes-top-2000-companies}{link to dataset}          & csv                 & 2.000                & 9                & Company, Country, Sales, Profits, Assets,   Market Value, Sector, Industry                                                                                                  \\
		dataworld (dw)& \href{https://www.data.world/youngx62/worlds-largest-companies-by-revenue}{link to dataset} & csv                  & 1.924                & 10               & Global Rank, Company, Sales, Profits, Assets, Market Value, Country,   Continent, Latitude, Longitude                                                                       \\
		dbpedia   & Query provided in Appendix                                      & json   &  3.986                &         11         & Name, industry\_label, domain,   founding\_year, ceos, no\_emp (MV), country (MV), location (MV),   revenue (MV), income (MV), assets (MV)    \\
	\end{tabularx}
\caption[Dataset Overview]%
{Dataset Overview. \small\medspace\medspace All dataset only refer to the class "Company", * For hyperlinks pls refer,\textsuperscript{1}\# of Entities, \textsuperscript{2}\# of Attributes, \textsuperscript{3} Number of filtered companies from the original dataset might be smaller than this number, because the final XML was extracted from the previous XML by mathching the filtered names (company with the same name might exist also in excluded category).}

\end{table}

%TODO normalize case
\begin{table}[]
	\small
	\label{tab:attribute-mapping}
	\begin{tabularx}{\textwidth}{mlmn}
		Class Name   & Attribute   Name            & Attribute Type & Contained in DS... \\\hline
		&&&\\
		Company      & name                        & String         & Kaggle, Forbes,   dbpedia, dw              \\
		Company      & domain                      & String         & Kaggle, dbpedia                            \\
		Company & Year founded                & Integer        & Kaggle, dbpedia                            \\
		Company      & Industry                    & String/List    & Kaggle, Forbes, dbpedia                    \\
		Company      & Size\_range                 & Category       & Kaggle (can also be   derived for dbpedia) \\
		Company      & locality                    & String         & Kaggle, dbpedia                            \\
		Company      & Country                     & String         & Kaggle, Forbes, dw,   dbpedia              \\
		Company      & Linkedin url                & String         & Kaggle                                     \\
		Company      & Current   employee estimate & Integer        & Kaggle, dbpedia                            \\
		Company      & Total employee   estimate   & String         & Kaggle                                     \\
		Company      & Sales                       & Integer        & Forbes, dw, dbpedia                        \\
		Company      & Profits                     & Integer        & Forbes, dw, dbpedia                        \\
		Company      & Assets                      & Integer        & Forbes, dw, dbpedia                        \\
		Company      & Market Value                & Integer        & Forbes, dw, dbpedia                        \\
		Company      & sector                      & String         & Forbes                                     \\
		Company      & Global Rank                 & Integer        & Dw                                         \\
		Company      & Latitude                    & Decimal        & dw                                         \\
		Company      & Longitude                   & Decimal        & Dw                                         \\
		Company      & ceos                        & list           & Dbpedia                                   
	\end{tabularx}
\caption{Attribute Mapping}
\end{table}



\section{Consolidated Schema \& Transformations}

The consolidated schema was created by hand. The following transformations were applied to the input datasets:\begin{enumerate}
	\item Monetary values were normalized to the same base.
	\item The \textit{dbpedia} dataset came with currency information. The intention was to use this information via a mapping table to convert all monetary values to USD. However, it turend out that there was a huge amount of currencies involved and it was not clear of which data the exchange rate should be retrieved. Therefore the currency was kept as an additional attribute.
	\item A unique ID was generated for each record that was mapped to the target schema.
	\item ...
\end{enumerate} 


\chapter{Phase II - Identity Resolution}
\label{cha:identity-resolution}

\section{Gold Standard}
\label{sec:gold-standard-IR}

In order to create the gold standard we ran initial identity resolutions with two cheap and a more complex matching rule. With a threshold of 0.2 we used three different matching rules: \begin{enumerate}
	\item Jaccard-3-Grams on company names (with frequent tokens removed) (MR1)
	\item Levensthein Similarity on company names (with frequent tokens removed) (MR2)
	\item A combination of 1, 2, Longest Common Subsequence, and a token-based similarity (MR27)
	%TODO !! --> maybe use something different, add rule IDs?
\end{enumerate}
%TODO add file references?? File Gold Standard integration.
%TODO global matching
%TODO source for string matching https://ebookcentral.proquest.com/lib/unimannheim-ebooks/reader.action?docID=945451
The results were then combined into one file which for each individual correspondence outlined the similarity calculated by every similarity measure as well as the company names. Furthermore, an average of the three similarities was calculated. We then labeled the matches into matching record pairs, corner-case matches and non-matches, and non-matches.
Therefore, the correspondences were sorted by the average similarity. A high value indicated that all matching rules consider this correspondence a match. Correspondences were labeld as certain matches if the similarity scores had a high matching threshold $>$ 0.9 and an actual match was present. Non-matches in this area were labeled as corner cases. For $0.9 > avg. sim. > 0.7$ mathces were labeled as corner-cases. Afterwards the correspondences with the lowest avg. sim. were reviewed and labeled as non-matches or corner-case mathches. Then, the correspondences were in turn sorted by each individual similarity measure score and non-categorized correspondences were labeld as corner-cases above a threshold of 0.7.
To achieve the distribution according to the rule of thumb outlined in the lecture, which states to include 20\% matching record matching pairs, 30\% corner-case matches and non-matches (fuzzy), and 50\% non-matching record pairs a random sample out of the labled correspondences was drawn.
%TODO was their a problem to achieve this distribution?
The data was then split into train and test set using a python script, with a test size of 0.25 and stratified on the gold standard category.

%TODO Note on balance of gold standard, positives and negatives

After running several identity resolutions the correspondences were analyzed. False positive mathches were then added to the gold standard as further corner cases.

\section{Matching Rules}
\label{sec:matching-rules}

\subsection{General Setup}
The company name was the sole variable we could rely on during the identity resolution across all datasets. Therefore, we implemented several string-based comparators which are outlined in table \ref{table:comparators} and subsequently combined them to form different matching rules. We implemented a plethora of different matching rules, therefore we limit ourselves to the top 10 based on their F1-scores.
%TODO why these comparators - maybe put with challenges?
Every comparator had options to include certain pre- and post-processing steps. Preprocessing steps generally included lowercasing, removal of punctuation, and removal of whitespaces (the latter was omitted for token based similarity metrics). We also implemented post-processing capabilities. These included a threshold after which the similarity was set to zero, and an option to boost or penalize the similarity based on a certain threshold. For example, a similarity might be boosted up using a particular function above a threshold of 0.8 and penalized below. %TODO might be necessary to outline this a little bit more, which type of functions and why those



\begin{table}[b]
	\label{table:comparators}
	
	\begin{tabular}{lllccc}
		& \multicolumn{2}{l}{}                              & \multicolumn{2}{l}{Preprocessing}                   & \\
		ID & Similarity   measure       & Parameters           & \multicolumn{1}{l}{PWL\textsuperscript{1}} & \multicolumn{1}{l}{Rm FT\textsuperscript{2}} & Focus\\
		1  & Jaccard on ngrams          & n: ngram   length    & \checked & (\checked)                     & Overlap\\
		2  & Jaccard on tokens          &                      & *                       & (\checked)                       &Overlap\\
		3  & Levensthein                &                      & \checked & (\checked)                       &Typos / Edit-distance\\
		4  & Longest Common Subsequence & Normalization   Flag & \checked & (\checked)                       & Order\\
		5  & RogueN on Tokens \cite{lin_rouge_2004} & & \checked & -  & Overlap                      
	\end{tabular}

\caption[Comparator Overview]%
{Comparator Overview \small\medspace\medspace \textsuperscript{1}Lowercasing and removal of punctuation and whitespaces -  latter not removed for token-based similarity metrics, \textsuperscript{2}Removal of frequent tokens, * Preprocessing done by pre-implemented similarity measure, \checked \space used in comparator, (\checked) optional}

\end{table}

\subsection{Major Challenge - Named Entity Matching}

A challenge in our project has been the matching of company names due to their "noisyness". We found ourselves facing similar challenges as the Dutch Central Bank \footnote{https://medium.com/dnb-data-science-hub/company-name-matching-6a6330710334}. %TODO make proper bibliographic reference
As with named entities in general every data source has a different level of detail, different data quality, and use of abbreviations among others. Regarding this, our matching rules had to cater to the following challenges:
\begin{enumerate}
	\item \textbf{Company Name:} In part the name of the legal entity was used, in other cases the name of the group, and in other cases some abbreviation (e.g., Anheuser-Busch InBev Germany Holding GmbH vs. Anheuser-Busch InBev vs. AB InBev) or tokens of the name were omitted (e.g., Royal Dutch Shell vs. Shell).
	\item \textbf{Data Quality:} We had to cope with general data quality issues which are represented for example by typos.
	\item \textbf{Frequent tokens:} There are several tokens that have a higher frequency in company names. These include for example legal entity descriptors (limited, incorporated, ...), industy descriptors (bank, motors, pharmaceuticals, ...) and stop words (the, and, of ...). These may let names seem more similar than they actually are (General Motors vs. Hyundai Motors). We analyzed frequent tokens across our datasets and provided the matching rules with the option to remove frequent tokens.
	\item \textbf{Token order:} Company names of different companies might be composed of similar tokens in a different order. Token-based similarity metrics alone would classify such names as similar altough they are not (Commercial National Financial vs. National Financial Group). This means that token order matters.
\end{enumerate}

We adressed these challenges by combining comparators with different strengths in matching rules which we systematically evaluated at different final mathching thresholds. We also implemented some new similarity metrics to cater to our needs (Comparator 4 and 5).

\subsection{Evaluation}

We evaluated local and global matching strategies. In table XXX the best matching rule for each threshold is presented.


%TODO parir completeness


\section{Blockers}
\label{sec:blockers}

We used three different types of blockers (no blocker, symmetric, sorted neighborhood) and different blocking key generators. The following key generators were implemented, all with certain preprocessing options: \begin{enumerate}
	\item First letter of the company name.
	\item Qgrams and first letter of the company name. \cite{gravano_approximate_nodate} suggest this blocking technique to ensure a low degree of missed pairs while staying computationally efficient. %TODO run without first letter
	\item N starting characters of each company name token.
\end{enumerate}

Especially the last blocker was born out of our second challenge, the large dataset size of the kaggle dataset.


\section{Analysis of Errors}
\label{sec:errors}


\chapter{Phase III - Data Fusion}
\label{cha:data-fusion}


%TODO Style Table
\begin{table}[h]

\begin{center}
\begin{tabular*}{\textwidth}{@{\extracolsep{\fill}}>{\scriptsize}l|>{\scriptsize}c>{\scriptsize}c>{\scriptsize}c|>{\scriptsize}c>{\scriptsize}c>{\scriptsize}c>{\scriptsize}c} 
& \multicolumn{3}{>{\scriptsize}c|}{Baselines} & \multicolumn{4}{>{\scriptsize}c}{Decision Tree} \\\hline
Ontology & M(edian) & G(ood) & E(vil) & results & $\Delta$-M & $\Delta$-G & $\Delta$-E \\\hline\hline
\#301 & 0.825 & 0.877 & 0.877 & 0.855 & +0.030 & -0.022 & -0.022 \\\hline
\#302 & 0.709 & 0.753 & 0.753 & 0.753 & +0.044 & +0.000 & +0.000 \\\hline
\#303 & 0.804 & 0.860 & 0.891 & 0.816 & +0.012 & -0.044 & -0.075 \\\hline
\#304 & 0.940 & 0.961 & 0.961 & 0.967 & +0.027 & +0.006 & +0.006 \\\hline
\bfseries Average & \bfseries 0.820 & \bfseries 0.863 & \bfseries 0.871 & \bfseries 0.848 & \bfseries +0.028 & \bfseries -0.015 & \bfseries -0.023 

\end{tabular*}
\caption[Good vs. Evil]{Comparison between the Good and the Evil}
\label{tab:confonly}
\end{center}
\end{table}



If you cite something, do it in the following way. 
\begin{itemize}
	\item Conference Proceedings: This problem is typically addressed by approaches for selecting the optimal matcher based on the nature of the matching task and the known characteristics of the different matching systems. Such an approach is described in \cite{mochol08matcher}.
	\item Journal Article: S-Match, described in \cite{giunchiglia2008semanticmatching}, employs sound and complete reasoning procedures. Nevertheless, the underlying semantic is restricted to propositional logic due to the fact that ontologies are interpreted as tree-like structures.
	\item Book: According to Euzenat and Shvaiko \cite{euzenat07matcherbook}, we define a correspondence as follows.
\end{itemize}
These are some randomly chosen examples from other works. Take a look at the end of this thesis so see how the bibliography is included.



\bibliographystyle{plain}
\bibliography{thesis-ref}


\appendix

\chapter{Program Code / Resources}
\label{cha:appendix-a}

The source code, a documentation, some usage examples, and additional test results are available at ...

They as well as a PDF version of this thesis is also contained on the CD\chapter{Phase II - Identity Resolution}
\label{cha:intro}-ROM attached to this thesis.

\chapter{Further Experimental Results}
\label{cha:appendix-b}

In the following further experimental results are ...


\newpage


\pagestyle{empty}


\section*{Ehrenw\"ortliche Erkl\"arung}
Ich versichere, dass ich die beiliegende Master-/Bachelorarbeit ohne Hilfe Dritter
und ohne Benutzung anderer als der angegebenen Quellen und Hilfsmittel
angefertigt und die den benutzten Quellen w\"ortlich oder inhaltlich
entnommenen Stellen als solche kenntlich gemacht habe. Diese Arbeit
hat in gleicher oder \"ahnlicher Form noch keiner Pr\"ufungsbeh\"orde
vorgelegen. Ich bin mir bewusst, dass eine falsche Er- kl\"arung rechtliche Folgen haben
wird.
\\
\\

\noindent
Mannheim, den 31.08.2014 \hspace{4cm} Unterschrift

\end{document}
