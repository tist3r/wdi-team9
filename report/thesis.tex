% do not change these two lines (this is a hard requirement
% there is one exception: you might replace oneside by twoside in case you deliver 
% the printed version in the accordant format
\documentclass[11pt,titlepage,oneside,openany]{article}
\usepackage{times}

%\usepackage{titlesec}
%\titleformat{\chapter}[display]   
%{\normalfont\huge\bfseries}{\chaptertitlename\
%	\thechapter}{20pt}{\Huge}   
%\titlespacing*{\chapter}{0pt}{-50pt}{40pt}


\usepackage{graphicx}
\usepackage{latexsym}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{wasysym}

\usepackage[hyphens]{url}

\usepackage{ntheorem}

% \usepackage{paralist}
\usepackage{tabularx}


\usepackage[table,xcdraw]{xcolor}
\usepackage{lscape}
\usepackage{afterpage}
\usepackage{rotating}
\usepackage{makecell}
\usepackage{hyperref}
\usepackage{arydshln}

% this packaes are useful for nice algorithms
\usepackage{algorithm}
\usepackage{algorithmic}

% well, when your work is concerned with definitions, proposition and so on, we suggest this
% feel free to add Corrolary, Theorem or whatever you need
\newtheorem{definition}{Definition}
\newtheorem{proposition}{Proposition}

%table spacing
\newcolumntype{b}{X}
\newcolumntype{n}{>{\hsize=2.2\hsize}X}
\newcolumntype{s}{>{\hsize=.3\hsize}X}
\newcolumntype{m}{>{\hsize=.5\hsize}X}


%\newcommand{\mychapter}[2]{
%	\setcounter{chapter}{#1}
%	\setcounter{section}{0}
%	\chapter{#2}
%	%\addcontentsline{toc}{chapter}{#2}
%}
% its always useful to have some shortcuts (some are specific for algorithms
% if you do not like your formating you can change it here (instead of scanning through the whole text)
\renewcommand{\algorithmiccomment}[1]{\ensuremath{\rhd} \textit{#1}}
\def\MYCALL#1#2{{\small\textsc{#1}}(\textup{#2})}
\def\MYSET#1{\scshape{#1}}
\def\MYAND{\textbf{ and }}
\def\MYOR{\textbf{ or }}
\def\MYNOT{\textbf{ not }}
\def\MYTHROW{\textbf{ throw }}
\def\MYBREAK{\textbf{break }}
\def\MYEXCEPT#1{\scshape{#1}}
\def\MYTO{\textbf{ to }}
\def\MYNIL{\textsc{Nil}}
\def\MYUNKNOWN{ unknown }
% simple stuff (not all of this is used in this examples thesis
\def\INT{{\mathcal I}} % interpretation
\def\ONT{{\mathcal O}} % ontology
\def\SEM{{\mathcal S}} % alignment semantic
\def\ALI{{\mathcal A}} % alignment
\def\USE{{\mathcal U}} % set of unsatisfiable entities
\def\CON{{\mathcal C}} % conflict set
\def\DIA{\Delta} % diagnosis
% mups and mips
\def\MUP{{\mathcal M}} % ontology
\def\MIP{{\mathcal M}} % ontology
% distributed and local entities
\newcommand{\cc}[2]{\mathit{#1}\hspace{-1pt} \# \hspace{-1pt} \mathit{#2}}
\newcommand{\cx}[1]{\mathit{#1}}
% complex stuff
\def\MER#1#2#3#4{#1 \cup_{#3}^{#2} #4} % merged ontology
\def\MUPALL#1#2#3#4#5{\textit{MUPS}_{#1}\left(#2, #3, #4, #5\right)} % the set of all mups for some concept
\def\MIPALL#1#2{\textit{MIPS}_{#1}\left(#2\right)} % the set of all mips





\begin{document}


\pagenumbering{roman}
% lets go for the title page, something like this should be okay
\begin{titlepage}
	\vspace*{2cm}
  \begin{center}
   {\Large The Extremes of Good and Evil\\}
   \vspace{2cm} 
   {Master Thesis\\}
   \vspace{2cm}
   {presented by\\
    Earl Hickey \\
    Matriculation Number 9083894\\
   }
   \vspace{1cm} 
   {submitted to the\\
    Data and Web Science Group\\
    Prof.\ Dr.\ Right Name Here\\
    University of Mannheim\\} \vspace{2cm}
   {August 2014}
  \end{center}
\end{titlepage} 

% no lets make some add some table of contents
\newpage

\listoftables

% evntuelly you might add something like this
% \listtheorems{definition}
% \listtheorems{proposition}

\newpage


% okay, start new numbering ... here is where it really starts
\pagenumbering{arabic}

\section{Phase I - Data Translation}
\label{cha:data-translation}

 

\subsection{Use Case \& Data Profiling}

\begin{table}[!htbp]
	%TODO dbpedia query
	
	\small
	\begin{tabularx}{\textwidth}{mscccn}
		Dataset   & Source                                                       & Format                & \#E\textsuperscript{1}       & \#A \textsuperscript{2} & List Of Attributes                                                                                                                                                          \\\hline
		&&&&&\\
		kaggle (kaggle\_f)    & \href{https://www.kaggle.com/kaleab1/companies}{link to dataset}                        & csv                  & \makecell[ct]{7.1M\\(491.830\textsuperscript{3})}                & 11               & ID, name, domain, year founded (MV), industry, size range, locality   (MV), country (MV), linkedin url, current employee estimate, total   employee estimate \\
		forbes    &\href{https://www.kaggle.com/ash316/forbes-top-2000-companies}{link to dataset}          & csv                 & 2.000                & 9                & Company, Country, Sales, Profits, Assets,   Market Value, Sector, Industry                                                                                                  \\
		dataworld (dw)& \href{https://www.data.world/youngx62/worlds-largest-companies-by-revenue}{link to dataset} & csv                  & 1.924                & 10               & Global Rank, Company, Sales, Profits, Assets, Market Value, Country,   Continent, Latitude, Longitude                                                                       \\
		dbpedia   & Query provided in Appendix                                      & json   &  3.986                &         11         & Name, industry\_label, domain,   founding\_year, ceos, no\_emp (MV), country (MV), location (MV),   revenue (MV), income (MV), assets (MV)    \\
	\end{tabularx}
	\caption[Dataset Overview]%
	{Dataset Overview. \small\medspace\medspace All dataset only refer to the class "Company", * For hyperlinks pls refer,\textsuperscript{1}\# of Entities, \textsuperscript{2}\# of Attributes, \textsuperscript{3} Number of filtered companies from the original dataset might be smaller than this number, because the final XML was extracted from the previous XML by mathching the filtered names (company with the same name might exist also in excluded category).}
	\label{tab:dataset-overview}
	
\end{table}


The goal of the project is to aggregate company information from several sources. To this end we used the suggestions from the project into slides as a foundation for our use case. We also included an additional source and amended the dbpedia query to extract further relevant information. Thus, the relevent entity will be a company.
We relied on 4 different datasources which are profiled in Table \ref{tab:dataset-overview}. In order to being able to process the kaggle dataset, we had to filter it down. To this end, we used the "size range" attribute and only kept the categories "10001+", "5001 - 10000", "1001 - 5000", "501 - 1000", "201 - 500", "51 - 200". This is a valid approach since the forbes dataset contains data about the 2000 largest companies in the world, and the dataworld (dw) dataset is also called "largest companies", containing only companies of a certain size.





%TODO normalize case
\begin{table}[t]
	\small
	\label{tab:attribute-mapping}
	\begin{tabularx}{\textwidth}{llll}
		Class Name   & Attribute   Name            & Attribute Type & Contained in DS... \\\hline
		&&&\\
		Company      & name                        & String         & Kaggle, Forbes,   dbpedia, dw              \\
		Company      & domain                      & String         & Kaggle, dbpedia                            \\
		Company & Year founded                & Integer        & Kaggle, dbpedia                            \\
		Company      & Industry                    & String/List    & Kaggle, Forbes, dbpedia                    \\
		Company      & Size\_range                 & Category       & Kaggle \\
		Company      & locality                    & String         & Kaggle, dbpedia                            \\
		Company      & Country                     & String         & Kaggle, Forbes, dw,   dbpedia              \\
		Company      & Linkedin url                & String         & Kaggle                                     \\
		Company      & Current   employee estimate & Integer        & Kaggle, dbpedia                            \\
		Company      & Total employee   estimate   & String         & Kaggle                                     \\
		Company      & Sales                       & Integer        & Forbes, dw, dbpedia                        \\
		Company      & Profits                     & Integer        & Forbes, dw, dbpedia                        \\
		Company      & Assets                      & Integer        & Forbes, dw, dbpedia                        \\
		Company      & Market Value                & Integer        & Forbes, dw, dbpedia                        \\
		Company      & sector                      & String         & Forbes                                     \\
		Company      & Global Rank                 & Integer        & Dw                                         \\
		Company      & Latitude                    & Decimal        & dw                                         \\
		Company      & Longitude                   & Decimal        & Dw                                         \\
		Company      & ceos                        & list           & Dbpedia                                   
	\end{tabularx}
\caption{Attribute Mapping}
\end{table}



\subsection{Consolidated Schema \& Transformations}

The consolidated schema was created by hand. Because we only considered one entity we were able to just add additional top level fields for each attribute. We created lists for the attribute industry and CEOs. The following transformations were applied to the input datasets:\begin{enumerate}
	\item Monetary values were normalized to the same base (1s).
	\item For forbes, and dw USD was added as the currency. For dbpedia the currency was parsed from the datatype annotation. Since the \textit{dbpedia} dataset came with currency information we intended to use this information via a mapping table to convert all monetary values to USD. However, it turned out that there was a huge amount of currencies involved and it was not clear from which date the exchange rate should be retrieved. Therefore the currency was kept as an additional attribute.
	\item A unique ID was generated for each record that was mapped to the target schema.
	\item Missing values in dw were partly encoded as \#N/A which confused the parse\_number function. An if clause was implemented that checked for "\#" and in these cases did not put a value, and otherwise the original value.
	\item For dbpedia the pipe concatenated fields ceos and industry were tokenized and only distinct values transfered to the respective list node.
\end{enumerate} 



\section{Phase II - Identity Resolution}
%\label{cha:identity-resolution}

\subsection{Gold Standard}
\label{sec:gold-standard-IR}

In order to create the gold standard we ran initial identity resolutions with two cheap and a more complex matching rule. With a threshold of 0.2 we used three different matching rules. (1) Jaccard-3-Grams on company names (with frequent tokens removed) (MR1), (2) Levensthein Similarity on company names (with frequent tokens removed) (MR2), (3) A combination of 1, 2, Longest Common Subsequence (LCS), and a token-based similarity, Rogue (MR27).

%TODO source for string matching https://ebookcentral.proquest.com/lib/unimannheim-ebooks/reader.action?docID=945451
The results were combined in one file, and an average similarity of the three matching rules was calculated. Afterwards, the correspondences were in turn sorted by each of the four similarities. Matches with a similarity $>$ 0.9 were labeled as sure-matches, non-matches with such a similarity as corner cases. In the range of 0.9 $> sim >$ 0.7 Correspondences were labeld as corner case matches or non-matches. The correspondences with lowest similarity score were also reviewed. Non-matches were labeled as sure non-matches and the few matches that still were present in this area were labeled as corner-cases. By repeating this procedure for each of the four similarity scores we had a broad coverage of different correspondences. To achieve the distribution according to the rule of thumb outlined in the lecture, which states to include 20\% matching record matching pairs, 30\% corner-case matches and non-matches (fuzzy), and 50\% non-matching record pairs a random sample out of the labled correspondences was drawn.
%TODO was their a problem to achieve this distribution?
The data was then split into train and test set using a python script, with a test size of 0.25 and stratified on the gold standard category (sure match, corner case, sure non-match).

%TODO Note on balance of gold standard, positives and negatives

After running several identity resolutions the correspondences were analyzed. False positive matches were then added to the gold standard as further corner cases. Moreover, the group size distribution was analyzed. Large groups indicated false postitive matches or duplicates. From several of these groups the true and some false matches were collected and added to the gold standard as as corner-cases, trying to keep the number of non-matches below 70\%. Afterwards, the goldstandard was reviewed for duplicates which were then removed.

\subsection{Matching Rules}
\label{sec:matching-rules}

\subsubsection{General Setup}
The company name was the sole variable we could rely on during the identity resolution across all datasets. Therefore, we implemented several string-based comparators which are outlined in table \ref{table:comparators} and subsequently combined them to form different matching rules. We implemented 27 different matching rules%
\footnote{Please refer to the class Comparators.MATCHING\_RULES.java for an overview of the concrete combination of comparators, weights, pre- and postprocessing.}%
, but limit the evaluation to the best and worst performing based on their F1-scores.

 Every comparator had options to include certain pre- and post-processing steps. Preprocessing steps generally included lowercasing, removal of punctuation, and removal of whitespaces (the latter was omitted for token based similarity metrics). Optionally, frequent tokens (FTs) could be removed. Moreover, different optional post-processing capabilities were implemented. These included a threshold after which the similarity was set to zero, and an option to boost or penalize the similarity based on a certain threshold. For example, a similarity might be boosted up using a particular function%
\footnote{To review the "boosting" functions please refer to Comparators.AbstractT9Comparator.java [double boost(double)]}%
 above a threshold of 0.8 and penalized below. The idea behind this is that if a similarity measure reaches a certain threshold a match becomes more likely although the score might be below the final matching threshold. To increase the probability for this match to be included the similarity should be increased. Conversly, if the similarity is below a certain threshold a match becomes more unlikely and the similarity score should be further penalized to have a higher impact on the final score. For example consider C5 for "Royal Dutch Shell" and "Shell" with a similarity of 0.67. One might say that this consitutes a high enough similarity for this comparator to be considered a match. Thus, one could set a boosting threshold of e.g. 0.6 meaning that $(sim - 0.6)^3/2*boostFactor$\footnote{This example considers the X3 boost function. Others include root-like or exponential functions.} would be added on top of the similarity. The boosting functions are designed to have a small impact close to the threshold and a bigger impact further away. 
An IR was conducted for each dataset against the \textit{kaggle\_f} dataset, because it has the largest amount of entities and is thus likely to yield a sufficient amount of correspondences as required.


\begin{table}[t]
	\small
	\begin{tabular}{lllcc}
		& \multicolumn{2}{l}{}                              & \multicolumn{2}{l}{Preprocessing}                   \\
		ID & Similarity   measure       & Parameters           & \multicolumn{1}{l}{PWL\textsuperscript{1}} & \multicolumn{1}{l}{Rm FT\textsuperscript{2}} \\\hline
		C1  & Jaccard on ngrams          & n: ngram   length    & \checked & (\checked)              \\
		C2  & Jaccard on tokens          &                      & *                       & (\checked)                       \\
		C3  & Levensthein                &                      & \checked & (\checked)                       \\
		C4  & Longest Common Subsequence & Normalization   Flag & \checked & (\checked)                       \\
		C5  & RogueN on Tokens \cite{lin_rouge_2004} & & \checked & -                      
	\end{tabular}

\caption[Comparator Overview]%
{Comparator Overview \small\medspace\medspace \textsuperscript{1}Lowercasing and removal of punctuation and whitespaces -  latter not removed for token-based similarity metrics, \textsuperscript{2}Removal of frequent tokens, * Preprocessing done by pre-implemented similarity measure, \checked \space used in comparator, (\checked) optional}
\label{table:comparators}

\end{table}

\subsubsection{Major Challenge - Named Entity Matching}
\label{sec:challenges}

The matching of company names has been a challenge in our project due to their "noisyness". We found ourselves facing similar challenges as the Dutch Central Bank \footnote{https://medium.com/dnb-data-science-hub/company-name-matching-6a6330710334}. %TODO make proper bibliographic reference
As with named entities in general every data source has a different level of detail, different data quality, and use of abbreviations among others. Regarding this, our matching rules had to cater to the following challenges:
\paragraph{Company name: }In part the name of the legal entity was used, in other cases the name of the group, and in other cases some abbreviation (e.g., Anheuser-Busch InBev Germany Holding GmbH vs. Anheuser-Busch InBev vs. AB InBev) or tokens of the name were omitted (e.g., Royal Dutch Shell vs. Shell). To address the first we implemented a Longest Common Subsequence (LCS) Comparator (C4). We used this over a Longest Common Substring to capture the described cases. For the latter we implented C5 which is a token based comparator. However, it uses a more favorable normalization that caters to the "Shell" example. Instead of normalizing with the distinct set of tokens (aka Jaccard) it calculates the overlap for each name over the number of tokens from it and then takes the average of both.

\paragraph{Data Quality:} We had to cope with general data quality issues which are represented for example by typos. Levenstheing similarity (C3) caters to this.

\paragraph{Frequent tokens:} There are several tokens that have a higher frequency in company names. These include for example legal entity descriptors (limited, incorporated, ...), industy descriptors (bank, motors, pharmaceuticals, ...) and stop words (the, and, of ...). These may let names seem more similar than they actually are (General Motors vs. Hyundai Motors). We analyzed frequent tokens across our datasets and provided the matching rules with the option to remove frequent tokens. However, with this option you also introduce the problem that "General Electric" and "General Motors" now have a similarity of 1. So this option has to be considered with caution and in combination with other metrics.

\paragraph{Token order:} Company names of different companies might be composed of similar tokens in a different order. Token-based similarity metrics alone would classify such names as similar altough they are not (Commercial National Financial vs. National Financial Group). This means that token order matters. The LCS Comparator and Levensthein also take this into account.

%TODO spacing required
In sum, we adressed these outlined challenges by combining comparators with different strengths in matching rules which we systematically evaluated at different final matching thresholds. We also implemented some new similarity metrics to cater to our needs (Comparator 4 and 5).



\subsubsection{Evaluation of Matching Rules}

We evaluated the thresholds 0.7, 0.8, 0.85, 0.875, and 0.9 for local matching strategies, becuase our analysis indicated that \textit{dbpedia} and \textit{kaggle\_f} included duplicated. While the precision of the matching rules usually increased with higher thresholds, the recall usually decreased%
\footnote{While the sample from table \ref{tab:mr-performance} does not reflect this trend, it was clearly visible in the over 400 experiments we conducted.}.%
This is intuitive since the easy matches have high similarity scores, while more ambigous matches although having a highish similarity, fall behind the easy matches and are thus excluded at higher thresholds. An exception were the learned matching rules, that changed their model at each threshold. The trade-off then depends on the use-case at hand. Considering F1-Scores, the matching threshold of 0.85 usually outperformed the other thresholds. However, considering the following step of data fusion, we favored higher precision over recall, which in turn was not always the MR with the highest F1-Score.

In general the F1-Scores of the \textit{dw - kaggle\_f} IRs fell behind the other IRs and \textit{dbpedia -kaglle\_f} yielded the best results. Simple MRs (i.e. MRs that might only consider Jaccard N-Grams or Levensthein similarity) were not able to outperform more complex matching rules that made use of a combination of the different similarity measures outlined in \ref{sec:challenges}. For example MR2 (dbpedia+kaggle\_f(0,85)) which consideres Jaccard 3-grams without removing FT achieves a high precision of 1 but lacks behind the other MR in terms of recall and at a threshold of 0.85 is the worst MR for the particular dataset combination. 

The top performing matching rules MR21 (LC), 26(pruned tree), 27(LC), 29(LC) all combine all but C2 in different ways.\footnote{LC: Linear Combination}%
They make use of different weights and pre-and post-processing options.

For example, MR26  is a pruned tree algorithm that removes FT for C1 and C3. All Comparators have a post-processing threshold between 0.3 and 0.4 where they are reduced to zero and make use of different boosting schemes. C4 and C5 have lower boosting thresholds because their similarity score might be lower, even if encountering a true positive. In table \ref{tab:mr-performance} can be seen that MR31 which is MR26 without post-processing performs worse, which speaks for the implemented post-processing options.

The other top performing MRs tested different post-processing options and different comparator weights. A possible improvement in this area could be a more systematic evaluation of the different post-processing options that were implemented to validate their effectiveness. Also learning these parameter settings might be an option.




\begin{table}[]
	\small
	\begin{tabular}{llcllllll}
		\textbf{DS Comb   + Thresh} & \multicolumn{1}{c}{\textbf{MR}} & \textbf{B\textsuperscript{$\star$}} & \multicolumn{1}{c}{\textbf{P}} & \multicolumn{1}{c}{\textbf{R}} & \multicolumn{1}{c}{\textbf{F1}} & \multicolumn{1}{c}{\textbf{\#Corr}} & \multicolumn{1}{c}{\textbf{Time}} & \multicolumn{1}{c}{\textbf{RR\textsuperscript{$\bullet$}}} \\\hline
		dbpedia+kaggle\_f(0,9)   & 27 & S        & 0,93 & 0,92 & 0,93 & 5643 & 16:56 & 0,9937 \\
		\rowcolor[HTML]{FFFFCC} 
		dbpedia+kaggle\_f(0,875) & 29 & S        & 0,98 & 0,91 & 0,94 & 3966 & 17:57 & 0,9937 \\
		dbpedia+kaggle\_f(0,85)  & 21 & S        & 0,93 & 0,96 & 0,95 & 5677 & 14:53 & 0,9937 \\
		\rowcolor[HTML]{FCCCC8} 
		dbpedia+kaggle\_f(0,85)  & 2  & S        & 1,00 & 0,59 & 0,74 & 2549 & 05:10 & 0,9937 \\
		\rowcolor[HTML]{DDEBF7} 
		dbpedia+kaggle\_f(0,875) & 21 & SNB(20)  & 1,00 & 0,17 & 0,30 & 755  & 00:48 & 0,9997 \\
		\rowcolor[HTML]{DDEBF7} 
		dbpedia+kaggle\_f(0,875) & 21 & SNB(100) & 0,98 & 0,45 & 0,61 & 2019 & 03:57 & 0,9984 \\
		\rowcolor[HTML]{DDEBF7} 
		dbpedia+kaggle\_f(0,875) & 21 & S (4g)   & 0,98 & 0,89 & 0,93 & 4437 & 18:40 & 0,9915 \\\hline\hline
		\rowcolor[HTML]{FFFFCC} 
		dw+kaggle\_f(0,9)        & 29 & S        & 0,94 & 0,73 & 0,83 & 2086 & 05:54 & 0,9941 \\
		dw+kaggle\_f(0,875)      & 29 & S        & 0,92 & 0,80 & 0,86 & 2493 & 05:54 & 0,9941 \\
		dw+kaggle\_f(0,85)       & 26 & S        & 0,91 & 0,88 & 0,89 & 4197 & 05:22 & 0,9941 \\
		\rowcolor[HTML]{FCCCC8} 
		dw+kaggle\_f(0,85)       & 15 & S        & 1,00 & 0,36 & 0,52 & 1575 & 04:27 & 0,9941 \\\hline\hline
		\rowcolor[HTML]{FFFFCC} 
		forbes+kaggle\_f(0,9)    & 26 & S        & 0,94 & 0,83 & 0,88 & 2712 & 07:58 & 0,9936 \\
		\rowcolor[HTML]{E7E6E6} 
		forbes+kaggle\_f(0,9)    & 31 \textsuperscript{$\divideontimes$} & S        & 0,93 & 0,73 & 0,82 & 3163 & 07:18 & 0,9936 \\
		forbes+kaggle\_f(0,875)  & 21 & S        & 0,91 & 0,84 & 0,87 & 3289 & 05:16 & 0,9936 \\
		forbes+kaggle\_f(0,85)   & 24 & S        & 0,94 & 0,86 & 0,90 & 3254 & 06:30 & 0,9936 \\
		\rowcolor[HTML]{FCCCC8} 
		forbes+kaggle\_f(0,85)   & 7  & S        & 1,00 & 0,01 & 0,03 & 1549 & 12:24 & 0,9936                       
	\end{tabular}
\caption[Matching Rule and Blocker Performance Overview (Excerpt)]{Matching Rule and Blocker Performance Overview (Excerpt) \medspace\small Yellow: Correspondences used for data fusion. Red: Worst MR at Threshold 0.85. Blue: Blocker comparison. \textsuperscript{$\star$}S (Standard Blocker) - 3 first letters of each token, S(4g) - 4grams, SNB(n) (Sorted Neighborhood Blocker). \textsuperscript{$\bullet$}Reduction Ratio. \textsuperscript{$\divideontimes$}Same as MR26 without post-processing.}
\label{tab:mr-performance}

\end{table}
When analyzing the group size distribution of the correspondence sets which are put forward to data fusion, several large groups can be identified (Table \ref{tab:group-size-dist}). While this is no ideal for data fusion, other group size distributions performed even worse, so that we have to use this imperfect correspondence set in the absence of the possibility for global matching.


\begin{table}[]
	\centering
	\small
	\begin{tabular}{lrrrrrrr}
		\textbf{\{MR\}\_\{B\}\_\{Thresh\}\_\{ds1\}\_\{ds2\}} & \multicolumn{1}{c}{\textbf{2}} & \multicolumn{1}{c}{\textbf{3-5}} & \multicolumn{1}{c}{\textbf{6-10}} & \multicolumn{1}{c}{\textbf{11-20}} & \multicolumn{1}{c}{\textbf{21-30}} & \multicolumn{1}{c}{\textbf{30+}} & \multicolumn{1}{c}{\textbf{PC\textsuperscript{$\star$}}} \\\hline
		29\_10\_87\_dbpedia\_kaggle\_f              & 1376                           & 710                              & 75                                & 10                                 & 2                                  & 0                                & 1                               \\
		29\_10\_9\_dw\_kaggle\_f                    & 729                            & 426                              & 43                                & 3                                  & 0                                  & 0                                & 1                               \\
		26\_10\_90\_forbes\_kaggle\_f               & 787                            & 508                              & 81                                & 9                                  & 1                                  & 0                                & 1                               \\\hline
		\multicolumn{1}{c}{}                        & \multicolumn{1}{c}{61\%}       & \multicolumn{1}{c}{35\%}         & \multicolumn{1}{c}{4\%}           & \multicolumn{1}{c}{0\%}            & \multicolumn{1}{c}{0\%}            & \multicolumn{1}{c}{0\%}          & \multicolumn{1}{c}{}           
	\end{tabular}
	\caption[Group Size Distribution of Correspondences used for Data Fusion]{Group Size Distribution of Correspondences used for Data Fusion. \small\textsuperscript{$\star$}Pair Completeness after Blocking}
	\label{tab:group-size-dist}
\end{table}


\subsection{Blockers}
\label{sec:blockers}
%TODO parir completeness
%SNB 20 - 31%
%SNB 100 - 50%
%4grams 97%
We intended to use three different types of blockers (no blocker, symmetric, sorted neighborhood) and different blocking key generators. The following key generators were implemented, all with certain preprocessing options. (1) First letter of the company name, (2) Qgrams(\cite{gravano_approximate_nodate} suggest this blocking technique to ensure a low degree of missed pairs while staying computationally efficient), (3) N starting characters of each company name token (in practice we mainly used 3 because 3 letter names are possible e.g. STX Group).

While we reduced the dataset size of the kaggle file significantly by filtering, we were still not able to evaluate a "No Blocker" and "3-gram blocker" against an IR that included \textit{kaggle\_f}although we increased the JVM Heap Space to 10 GB. However, we were able to circumvent this problem by using a standard blocker in combination with the the first 3 letter of each name token as keys, that provided a Pair Completeness of 100\%. We also evaluated 4-grams. Table \ref{tab:mr-performance} includes different blockers for \textit{dbpedia + kaggle\_f} with MR21 as reference, the other dataset combinations obviously behaved similarly in terms of runtime and pair completeness. Using the Sorted Neighborhood Blocker (SNB) in combination with a 3-gram key generator yieled very bad results in terms of recall. This was the case for a window size of 20 as well as larger ones, e.g. 100. The SNB was able to reach the highest reduction ratios and thus the best runtimes, but apparently missed pairs. This could be due to the fact that several blocks contained more than 100 blocked elements were present. SNB(20) had a pari completeness of 31\% SNB(100) 50\%, and S(4g) 97\%. The first 3 letters of each token generated fewer blocked pairs while still achieving a pair completeness of 100\%. The 4-grams achieved the lowest reduction ratio (still $>$99\%) and hence had the longest runtime.


\subsection{Analysis of Errors}
\label{sec:errors}
An example of a match with large group size for \textit{dbpedia\_kaggle\_f} is "Shift Inc." that matched with 19 companies named "shift" (similarity of 0.89). However, none of these actually represent a match, when taking urls and founding year into account. We did not incorporate these into the matching rules because they often have missing values.
A false positive match for example is "Volkswagen Group" and "volkswagen group uk ltd" (0,90). While they represent the same group, the latter represents the uk branch and thus a different entity. This is a problem that we had with other groups as well. Due to the high token overlap oftentimes they are considered a match.
A missed pair includes "Amrutanjan Healthcare" and "amrutanjan health care limited" (0.84). Because "health" and "care" are separate tokens in the latter one, the token-based comparator had difficulties to get to a high enough similarity.

\textbf{Summary. }While the removal of FT oftentimes was useful (e.g. "Telus Corporation","telus") it could be seen that it raised new problems by creating non-matching correspondences (e.g "Ams AG", "ams technologies ag"). We tried to remediate this by including more comparators that still considered FTs but were less sensible to them. LCS is able to "skip" them, and RogueToken does not penalize the company name with missing FT as much as JaccardToken. In general it could be seen that we were able to achieve reasonable results with this. The "subsidiary" problem could be partly addressed by having a country table and identifying tokens that refer to countries and giving them a stronger weight. For larger corporations including the url into the comparison might have helped, however because this attribute was also often missing other pairs would have been missed. A possible remediation would have been the removal of duplicates beforehand, to allow for global matching.



\section{Phase III - Data Fusion}
\label{cha:data-fusion}

For data fusion we used all of our datasets. Especially \textit{dbpedia} and \textit{kaggle\_f} had a good overlap, but also the other dataset provide some overlap. We focused on the follwing attributes: name, country, industries, sales amount, sales currency, current employees, and year founded.

We added the latest revision date of the datasets as provenance date, and increased the score of the \textit{forbes} dataset compared to the others, because we consider it a high fidelity source. This will be used with the conflict resolution function "Favour Source" for sales amount and sales currency.

Unfortunately, due to an error we were not able to evaluate the density of the datasets.\footnote{java.lang.NullPointerException: Cannot invoke "de.uni\_mannheim.informatik.dws.winter.model.DataSet.get()" because the return value of "de.uni\_mannheim.informatik.dws.winter.model.FusibleHashedDataSet.getSchema()" is null}%
The attribute consistency is outlined in table \ref{tab:conflict-resolution}, along with several thresholds and consistency measures. The table also outlines the conflict resolution functions we used. For example, Union was used for the industry list attribute. This is appropriate, because industry labels are rather vague and without a supporting taxonomy of other lookup table a more concrete overlap comparison for example is difficult. We decided to favor the forbes dataset for the sales amount, because we consider it a credible source. We used the same conflict resolution fucntion for the according currency symbol to keep the data consistent. For the company name we decided to take the longest string, as it might contain additional information such as the company type (i.e. ltd, AG, GmbH, ...).

\begin{table}[]
	\small
	\centering
	\begin{tabular}{llcc}
		\textbf{Attribute} & \textbf{Consistency}     & \multicolumn{1}{l}{\textbf{Threshold}} & \multicolumn{1}{l}{\textbf{Conflict Resolution Function}} \\\hline
		YEAR\_FOUNDED      & 0,98                  & +/- 5\%                                & Median                                                    \\
		INDUSTRY           & 0,36                  & \textgreater 1 common label            & Union                                                     \\
		COUNTRY            & 0,82                  & Levensthein \textgreater{}0.95         & Shortest String                                           \\
		CURRENT\_EMPLOYEES & 0,42                  & +/- 5\%                                & Average                                                   \\
		NAME               & 1,00                  & LCS \textgreater{}0.75                 & Longest String                                            \\
		SALES\_AMOUNT      & \multicolumn{1}{r}{-} & +/- 5\%                                & Favour Source (forbes)                                    \\
		SALES\_CURRENCY    & \multicolumn{1}{r}{-} & JaccardToken = 1                       & Favour Source (forbes)                                   
	\end{tabular}

\caption[Data Fusion Evaluation and Conflict Resolution Overview]{Data Fusion Evaluation and Conflict Resolution Overview}

\label{tab:conflict-resolution}
\end{table}

A gold standard with 15 samples was created based on entities from the forbes dataset. To this end the attributes used during fusion were manually reviewed against external sources (e.g. Statista, Wikipedia, and Bloomberg).


\section{Summary}

%TODO Style Table
\begin{table}[h]

\begin{center}
\begin{tabular*}{\textwidth}{@{\extracolsep{\fill}}>{\scriptsize}l|>{\scriptsize}c>{\scriptsize}c>{\scriptsize}c|>{\scriptsize}c>{\scriptsize}c>{\scriptsize}c>{\scriptsize}c} 
& \multicolumn{3}{>{\scriptsize}c|}{Baselines} & \multicolumn{4}{>{\scriptsize}c}{Decision Tree} \\\hline
Ontology & M(edian) & G(ood) & E(vil) & results & $\Delta$-M & $\Delta$-G & $\Delta$-E \\\hline\hline
\#301 & 0.825 & 0.877 & 0.877 & 0.855 & +0.030 & -0.022 & -0.022 \\\hline
\#302 & 0.709 & 0.753 & 0.753 & 0.753 & +0.044 & +0.000 & +0.000 \\\hline
\#303 & 0.804 & 0.860 & 0.891 & 0.816 & +0.012 & -0.044 & -0.075 \\\hline
\#304 & 0.940 & 0.961 & 0.961 & 0.967 & +0.027 & +0.006 & +0.006 \\\hline
\bfseries Average & \bfseries 0.820 & \bfseries 0.863 & \bfseries 0.871 & \bfseries 0.848 & \bfseries +0.028 & \bfseries -0.015 & \bfseries -0.023 

\end{tabular*}
\caption[Good vs. Evil]{Comparison between the Good and the Evil}
\label{tab:confonly}
\end{center}
\end{table}



If you cite something, do it in the following way. 
\begin{itemize}
	\item Conference Proceedings: This problem is typically addressed by approaches for selecting the optimal matcher based on the nature of the matching task and the known characteristics of the different matching systems. Such an approach is described in \cite{mochol08matcher}.
	\item Journal Article: S-Match, described in \cite{giunchiglia2008semanticmatching}, employs sound and complete reasoning procedures. Nevertheless, the underlying semantic is restricted to propositional logic due to the fact that ontologies are interpreted as tree-like structures.
	\item Book: According to Euzenat and Shvaiko \cite{euzenat07matcherbook}, we define a correspondence as follows.
\end{itemize}
These are some randomly chosen examples from other works. Take a look at the end of this thesis so see how the bibliography is included.



\bibliographystyle{plain}
\bibliography{thesis-ref}


\appendix


\newpage


\pagestyle{empty}


\section*{Ehrenw\"ortliche Erkl\"arung}
Ich versichere, dass ich die beiliegende Master-/Bachelorarbeit ohne Hilfe Dritter
und ohne Benutzung anderer als der angegebenen Quellen und Hilfsmittel
angefertigt und die den benutzten Quellen w\"ortlich oder inhaltlich
entnommenen Stellen als solche kenntlich gemacht habe. Diese Arbeit
hat in gleicher oder \"ahnlicher Form noch keiner Pr\"ufungsbeh\"orde
vorgelegen. Ich bin mir bewusst, dass eine falsche Er- kl\"arung rechtliche Folgen haben
wird.
\\
\\

\noindent
Mannheim, den 31.08.2014 \hspace{4cm} Unterschrift

\end{document}
