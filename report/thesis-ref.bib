
@book{dong_big_2015,
	address = {San Rafael, California},
	edition = {1.},
	series = {Synthesis {Lectures} on {Data} {Management}},
	title = {Big {Data} {Integration}},
	abstract = {The big data era is upon us: data are being generated, analyzed, and used at an unprecedented scale, and data-driven decision making is sweeping through all aspects of society. Since the value of data explodes when it can be linked and fused with other data, addressing the big data integration (BDI) challenge is critical to realizing the promise of big data. BDI differs from traditional data integration along the dimensions of volume, velocity, variety, and veracity. First, not only can data sources contain a huge volume of data, but also the number of data sources is now in the millions. Second, because of the rate at which newly collected data are made available, many of the data sources are very dynamic, and the number of data sources is also rapidly exploding. Third, data sources are extremely heterogeneous in their structure and content, exhibiting considerable variety even for substantially similar entities. Fourth, the data sources are of widely differing qualities, with significant differences in the coverage, accuracy and timeliness of data provided. This book explores the progress that has been made by the data integration community on the topics of schema alignment, record linkage and data fusion in addressing these novel challenges faced by big data integration. Each of these topics is covered in a systematic way: first starting with a quick tour of the topic in the context of traditional data integration, followed by a detailed, example-driven exposition of recent innovative techniques that have been proposed to address the BDI challenges of volume, velocity, variety, and veracity. Finally, it presents emerging topics and opportunities that are specific to BDI, identifying promising directions for the data integration community.},
	language = {eng},
	number = {40},
	publisher = {Morgan \& Claypool},
	author = {Dong, Xin Luna and Srivastava, Divesh},
	year = {2015},
	keywords = {Big data, Data integration (Computer science), Electronic books},
}

@book{doan_principles_2012,
	address = {Waltham, MA},
	title = {Principles of {Data} {Integration}},
	abstract = {Principles of Data Integration is the first comprehensive textbook of data integration, covering theoretical principles and implementation issues as well as current challenges raised by the semantic web and cloud computing. The book offers a range of data integration solutions enabling you to focus on what is most relevant to the problem at hand. Readers will also learn how to build their own algorithms and implement their own data integration application. Written by three of the most respected experts in the field, this book provides an extensive introduction to the theory and concepts underlying today's data integration techniques, with detailed, instruction for their application using concrete examples throughout to explain the concepts. This text is an ideal resource for database practitioners in industry, including data warehouse engineers, database system designers, data architects/enterprise architects, database researchers, statisticians, and data analysts; students in data analytics and knowledge discovery; and other data professionals working at the R\&D and implementation levels. Offers a range of data integration solutions enabling you to focus on what is most relevant to the problem at hand Enables you to build your own algorithms and implement your own data integration applications},
	language = {English},
	publisher = {Morgan Kaufmann},
	author = {Doan, AnHai and Halevy, Alon and Ives, Zachary},
	year = {2012},
	keywords = {Data integration (Computer science), COMPUTERS / Data Science / General, COMPUTERS / Database Administration \& Management},
}

@article{binette_almost_2022,
	title = {({Almost}) all of entity resolution},
	volume = {8},
	issn = {2375-2548},
	url = {https://www.science.org/doi/10.1126/sciadv.abi8021},
	doi = {10.1126/sciadv.abi8021},
	abstract = {Whether the goal is to estimate the number of people that live in a congressional district, to estimate the number of individuals that have died in an armed conflict, or to disambiguate individual authors using bibliographic data, all these applications have a common theme—integrating information from multiple sources. Before such questions can be answered, databases must be cleaned and integrated in a systematic and accurate way, commonly known as structured entity resolution (record linkage or deduplication). Here, we review motivational applications and seminal papers that have led to the growth of this area. We review modern probabilistic and Bayesian methods in statistics, computer science, machine learning, database management, economics, political science, and other disciplines that are used throughout industry and academia in applications such as human rights, official statistics, medicine, and citation networks, among others. Last, we discuss current research topics of practical importance.
          , 
            This article reviews entity resolution or record linkage in both statistics and computer science.},
	language = {en},
	number = {12},
	urldate = {2022-11-08},
	journal = {Science Advances},
	author = {Binette, Olivier and Steorts, Rebecca C.},
	month = mar,
	year = {2022},
	pages = {eabi8021},
	file = {Binette und Steorts - 2022 - (Almost) all of entity resolution.pdf:C\:\\Users\\schmi\\Zotero\\storage\\IXC7HLG7\\Binette und Steorts - 2022 - (Almost) all of entity resolution.pdf:application/pdf},
}

@article{navarro_guided_2001,
	title = {A guided tour to approximate string matching},
	volume = {33},
	issn = {0360-0300, 1557-7341},
	url = {https://dl.acm.org/doi/10.1145/375360.375365},
	doi = {10.1145/375360.375365},
	abstract = {We survey the current techniques to cope with the problem of string matching that allows errors. This is becoming a more and more relevant issue for many fast growing areas such as information retrieval and computational biology. We focus on online searching and mostly on edit distance, explaining the problem and its relevance, its statistical behavior, its history and current developments, and the central ideas of the algorithms and their complexities. We present a number of experiments to compare the performance of the different algorithms and show which are the best choices. We conclude with some directions for future work and open problems.},
	language = {en},
	number = {1},
	urldate = {2022-11-08},
	journal = {ACM Computing Surveys},
	author = {Navarro, Gonzalo},
	month = mar,
	year = {2001},
	pages = {31--88},
	file = {Navarro - 2001 - A guided tour to approximate string matching.pdf:C\:\\Users\\schmi\\Zotero\\storage\\PYMEQYQS\\Navarro - 2001 - A guided tour to approximate string matching.pdf:application/pdf},
}

@article{christophides_overview_2021,
	title = {An {Overview} of {End}-to-{End} {Entity} {Resolution} for {Big} {Data}},
	volume = {53},
	issn = {0360-0300, 1557-7341},
	url = {https://dl.acm.org/doi/10.1145/3418896},
	doi = {10.1145/3418896},
	abstract = {One of the most critical tasks for improving data quality and increasing the reliability of data analytics is
              Entity Resolution
              (ER), which aims to identify different descriptions that refer to the same real-world entity. Despite several decades of research, ER remains a challenging problem. In this survey, we highlight the novel aspects of resolving Big Data entities when we should satisfy more than one of the Big Data characteristics simultaneously (i.e., Volume and Velocity with Variety). We present the basic concepts, processing steps, and execution strategies that have been proposed by database, semantic Web, and machine learning communities in order to cope with the loose
              structuredness
              , extreme
              diversity
              , high
              speed,
              and large
              scale
              of entity descriptions used by real-world applications. We provide an end-to-end view of ER workflows for Big Data, critically review the pros and cons of existing methods, and conclude with the main open research directions.},
	language = {en},
	number = {6},
	urldate = {2022-11-08},
	journal = {ACM Computing Surveys},
	author = {Christophides, Vassilis and Efthymiou, Vasilis and Palpanas, Themis and Papadakis, George and Stefanidis, Kostas},
	month = nov,
	year = {2021},
	pages = {1--42},
	file = {Christophides et al. - 2021 - An Overview of End-to-End Entity Resolution for Bi.pdf:C\:\\Users\\schmi\\Zotero\\storage\\YLVG9XGW\\Christophides et al. - 2021 - An Overview of End-to-End Entity Resolution for Bi.pdf:application/pdf},
}

@inproceedings{gali_similarity_2016,
	title = {Similarity measures for title matching},
	doi = {10.1109/ICPR.2016.7899857},
	abstract = {In many web applications, users query a place name, a photo name, and other entity names using search words that include alternate spellings, abbreviations, and variants that are similar, but not identical to the title associated with the desired entity. Given two titles, an effective similarity measure should be able to determine whether the titles represent the same entity or not. In this paper, we evaluate 21 measures with the aim of detecting the most appropriate measure for matching the titles. Results show that Soft-TFIDF performs the best.},
	booktitle = {2016 23rd {International} {Conference} on {Pattern} {Recognition} ({ICPR})},
	author = {Gali, Najlah and Mariescu-Istodor, Radu and Fränti, Pasi},
	month = dec,
	year = {2016},
	keywords = {Biomedical measurement, Computer science, Computers, Frequency measurement, information retrieval, Information retrieval, Ontologies, similarity measures, title matching, web mining, Weight measurement},
	pages = {1548--1553},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\schmi\\Zotero\\storage\\85LJ4ZRE\\7899857.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\schmi\\Zotero\\storage\\L7MDSBRH\\Gali et al. - 2016 - Similarity measures for title matching.pdf:application/pdf},
}

@inproceedings{lin_rouge_2004,
	address = {Barcelona, Spain},
	title = {{ROUGE}: {A} {Package} for {Automatic} {Evaluation} of {Summaries}},
	shorttitle = {{ROUGE}},
	url = {https://aclanthology.org/W04-1013},
	urldate = {2022-11-21},
	booktitle = {Text {Summarization} {Branches} {Out}},
	publisher = {Association for Computational Linguistics},
	author = {Lin, Chin-Yew},
	month = jul,
	year = {2004},
	pages = {74--81},
	file = {Full Text PDF:C\:\\Users\\schmi\\Zotero\\storage\\3QBAWI8E\\Lin - 2004 - ROUGE A Package for Automatic Evaluation of Summa.pdf:application/pdf},
}

@inproceedings{gravano_approximate_2001,
	title = {Approximate string joins in a database (almost) for free},
	volume = {1},
	booktitle = {{VLDB}},
	author = {Gravano, Luis and Ipeirotis, Panagiotis G and Jagadish, Hosagrahar Visvesvaraya and Koudas, Nick and Muthukrishnan, Shanmugauelayut and Srivastava, Divesh and {others}},
	year = {2001},
	pages = {491--500},
}

@article{nijhuis_company_2022,
	title = {Company {Name} {Matching}},
	url = {https://medium.com/dnb-data-science-hub/company-name-matching-6a6330710334},
	abstract = {We have all been there: you have found two interesting datasets that could really supplement each other, but… you have no way of joining…},
	language = {en},
	urldate = {2022-12-04},
	journal = {Medium.com},
	author = {Nijhuis, Michiel},
	month = mar,
	year = {2022},
	file = {Snapshot:C\:\\Users\\schmi\\Zotero\\storage\\5CA4RDXE\\company-name-matching-6a6330710334.html:text/html},
}
